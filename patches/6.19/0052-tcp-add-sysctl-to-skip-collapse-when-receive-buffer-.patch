From c2647c3be98bc12813826a0a2bf5387a96aa4bbd Mon Sep 17 00:00:00 2001
From: Alexandre Frade <kernel@xanmod.org>
Date: Sat, 7 Feb 2026 23:43:47 -0500
Subject: [PATCH 52/180] tcp: add sysctl to skip collapse when receive buffer is
 full

Add net.ipv4.tcp_collapse_max_bytes sysctl to cap how much data
tcp_collapse() processes. When receive buffer exceeds the limit,
drop the tail instead of doing expensive coalescing that can cause
hundreds of milliseconds of latency spikes on fast connections.

Default 0 (disabled, stock behavior). No overhead unless explicitly set.

Original-author: Alexandre Frade
---
 include/net/netns/ipv4.h   |  1 +
 include/trace/events/tcp.h |  7 +++++++
 net/ipv4/sysctl_net_ipv4.c |  7 +++++++
 net/ipv4/tcp_input.c       | 36 ++++++++++++++++++++++++++++++++++++
 net/ipv4/tcp_ipv4.c        |  1 +
 5 files changed, 52 insertions(+)

diff --git a/include/net/netns/ipv4.h b/include/net/netns/ipv4.h
index 2dbd46fc4734..b14d4569144b 100644
--- a/include/net/netns/ipv4.h
+++ b/include/net/netns/ipv4.h
@@ -243,10 +243,11 @@ struct netns_ipv4 {
 	int sysctl_udp_wmem_min;
 	int sysctl_udp_rmem_min;
 
 	u8 sysctl_fib_notify_on_flag_change;
 	u8 sysctl_tcp_syn_linear_timeouts;
+	unsigned int sysctl_tcp_collapse_max_bytes;
 
 #ifdef CONFIG_NET_L3_MASTER_DEV
 	u8 sysctl_udp_l3mdev_accept;
 #endif
 
diff --git a/include/trace/events/tcp.h b/include/trace/events/tcp.h
index 6757233bd064..832031e64ce3 100644
--- a/include/trace/events/tcp.h
+++ b/include/trace/events/tcp.h
@@ -284,10 +284,17 @@ TRACE_EVENT(tcp_rcvbuf_grow,
 		  __entry->saddr_v6, __entry->daddr_v6,
 		  __entry->skaddr,
 		  __entry->sock_cookie)
 );
 
+DEFINE_EVENT(tcp_event_sk, tcp_collapse_max_bytes_exceeded,
+
+	TP_PROTO(struct sock *sk),
+
+	TP_ARGS(sk)
+);
+
 TRACE_EVENT(tcp_retransmit_synack,
 
 	TP_PROTO(const struct sock *sk, const struct request_sock *req),
 
 	TP_ARGS(sk, req),
diff --git a/net/ipv4/sysctl_net_ipv4.c b/net/ipv4/sysctl_net_ipv4.c
index a1a50a5c80dc..2de308e63e40 100644
--- a/net/ipv4/sysctl_net_ipv4.c
+++ b/net/ipv4/sysctl_net_ipv4.c
@@ -1614,10 +1614,17 @@ static struct ctl_table ipv4_net_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_dou8vec_minmax,
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
+	{
+		.procname	= "tcp_collapse_max_bytes",
+		.data		= &init_net.ipv4.sysctl_tcp_collapse_max_bytes,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_douintvec_minmax,
+	},
 	{
 		.procname	= "tcp_pingpong_thresh",
 		.data		= &init_net.ipv4.sysctl_tcp_pingpong_thresh,
 		.maxlen		= sizeof(u8),
 		.mode		= 0644,
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index a965bde56488..bd1e6c8e3af9 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -5801,10 +5801,11 @@ static bool tcp_prune_ofo_queue(struct sock *sk, const struct sk_buff *in_skb)
  * to stabilize the situation.
  */
 static int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+	struct net *net = sock_net(sk);
 
 	/* Do nothing if our queues are empty. */
 	if (!atomic_read(&sk->sk_rmem_alloc))
 		return -1;
 
@@ -5816,10 +5817,43 @@ static int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb)
 		tcp_adjust_rcv_ssthresh(sk);
 
 	if (tcp_can_ingest(sk, in_skb))
 		return 0;
 
+	/* For context and additional information about this patch, see the
+	 * blog post at
+	 *
+	 * sysctl:  net.ipv4.tcp_collapse_max_bytes
+	 *
+	 * If tcp_collapse_max_bytes is non-zero, attempt to collapse the
+	 * queue to free up memory if the current amount of memory allocated
+	 * is less than tcp_collapse_max_bytes.  Otherwise, the packet is
+	 * dropped without attempting to collapse the queue.
+	 *
+	 * If tcp_collapse_max_bytes is zero, this feature is disabled
+	 * and the default Linux behavior is used.  The default Linux
+	 * behavior is to always perform the attempt to collapse the
+	 * queue to free up memory.
+	 *
+	 * When the receive queue is small, we want to collapse the
+	 * queue.  There are two reasons for this: (a) the latency of
+	 * performing the collapse will be small on a small queue, and
+	 * (b) we want to avoid sending a congestion signal (via a
+	 * packet drop) to the sender when the receive queue is small.
+	 *
+	 * The result is that we avoid latency spikes caused by the
+	 * time it takes to perform the collapse logic when the receive
+	 * queue is large and full, while preserving existing behavior
+	 * and performance for all other cases.
+	 */
+	if (net->ipv4.sysctl_tcp_collapse_max_bytes &&
+		(atomic_read(&sk->sk_rmem_alloc) > net->ipv4.sysctl_tcp_collapse_max_bytes)) {
+		/* We are dropping the packet */
+		trace_tcp_collapse_max_bytes_exceeded(sk);
+		goto do_not_collapse;
+	}
+
 	tcp_collapse_ofo_queue(sk);
 	if (!skb_queue_empty(&sk->sk_receive_queue))
 		tcp_collapse(sk, &sk->sk_receive_queue, NULL,
 			     skb_peek(&sk->sk_receive_queue),
 			     NULL,
@@ -5834,10 +5868,12 @@ static int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb)
 	tcp_prune_ofo_queue(sk, in_skb);
 
 	if (tcp_can_ingest(sk, in_skb))
 		return 0;
 
+do_not_collapse:
+
 	/* If we are really being abused, tell the caller to silently
 	 * drop receive data on the floor.  It will get retransmitted
 	 * and hopefully then we'll have sufficient space.
 	 */
 	NET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index f8a9596e8f4d..e4040696ab0d 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -3618,10 +3618,11 @@ static int __net_init tcp_sk_init(struct net *net)
 	else
 		net->ipv4.tcp_congestion_control = &tcp_reno;
 
 	net->ipv4.sysctl_tcp_syn_linear_timeouts = 4;
 	net->ipv4.sysctl_tcp_shrink_window = 0;
+	net->ipv4.sysctl_tcp_collapse_max_bytes = 0;
 
 	net->ipv4.sysctl_tcp_pingpong_thresh = 1;
 	net->ipv4.sysctl_tcp_rto_min_us = jiffies_to_usecs(TCP_RTO_MIN);
 	net->ipv4.sysctl_tcp_rto_max_ms = TCP_RTO_MAX_SEC * MSEC_PER_SEC;
 
-- 
2.52.0

